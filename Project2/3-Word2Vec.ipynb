{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linn Cao Nguyen Phuong**\n",
    "\n",
    "Spring 2023\n",
    "\n",
    "2/21/2023\n",
    "\n",
    "CS 421\n",
    "\n",
    "Project 2: Python2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.0-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\linnxinh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim) (1.21.2)\n",
      "Collecting Cython==0.29.32\n",
      "  Using cached Cython-0.29.32-py2.py3-none-any.whl (986 kB)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\linnxinh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Collecting FuzzyTM>=0.4.0\n",
      "  Using cached FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Collecting pyfume\n",
      "  Using cached pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\linnxinh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\linnxinh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\linnxinh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\linnxinh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Collecting fst-pso\n",
      "  Using cached fst-pso-1.8.1.tar.gz (18 kB)\n",
      "Collecting simpful\n",
      "  Using cached simpful-2.9.0-py3-none-any.whl (30 kB)\n",
      "Collecting miniful\n",
      "  Using cached miniful-0.0.6.tar.gz (2.8 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.0.1-cp39-cp39-win_amd64.whl (96 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "Using legacy 'setup.py install' for fst-pso, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for miniful, since package 'wheel' is not installed.\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests, miniful, simpful, fst-pso, pyfume, smart-open, FuzzyTM, Cython, gensim\n",
      "    Running setup.py install for miniful: started\n",
      "    Running setup.py install for miniful: finished with status 'done'\n",
      "    Running setup.py install for fst-pso: started\n",
      "    Running setup.py install for fst-pso: finished with status 'done'\n",
      "Successfully installed Cython-0.29.32 FuzzyTM-2.0.5 certifi-2022.12.7 charset-normalizer-3.0.1 fst-pso-1.8.1 gensim-4.3.0 idna-3.4 miniful-0.0.6 pyfume-0.2.25 requests-2.28.2 simpful-2.9.0 smart-open-6.3.0 urllib3-1.26.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script normalizer.exe is installed in 'c:\\Users\\linnxinh\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts cygdb.exe, cython.exe and cythonize.exe are installed in 'c:\\Users\\linnxinh\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 21.2.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\linnxinh\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\linnxinh\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-5.13.0-py2.py3-none-any.whl (15.2 MB)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.13.0 tenacity-8.2.1\n"
     ]
    }
   ],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_files = []\n",
    "for dirpath, dirnames, filenames in os.walk('libraries'):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.py'):\n",
    "            py_files.append(os.path.join(dirpath,filename))\n",
    "\n",
    "# print(py_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('py_files.txt', 'w', encoding='utf-8', errors='replace') as f:\n",
    "    for py_file in py_files:\n",
    "        try:\n",
    "            with open(py_file, 'r', encoding='utf-8', errors='replace') as f_in:\n",
    "                f.write(f_in.read() + '\\n')\n",
    "        except OSError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 4606201\n",
      "Tokens: 11053140\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def tokenize_line(line):\n",
    "    line = line.lower()  \n",
    "    word_list = [word.strip(string.punctuation) for word in line.split()]\n",
    "    word_list = list(filter(None, word_list))\n",
    "    return word_list\n",
    "\n",
    "with codecs.open('py_files.txt', 'r', 'utf-8') as f:\n",
    "    lines_of_code = []\n",
    "    num_lines = 0\n",
    "    tokens = 0\n",
    "    for line in f:\n",
    "        if line.strip().startswith(\"#\"):\n",
    "            continue  \n",
    "        lines_of_code.append(tokenize_line(line))\n",
    "        num_lines += 1\n",
    "        tokens += len(tokenize_line(line))\n",
    "\n",
    "print(\"Number of lines:\", num_lines)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest words to \"for\":\n",
      "[('in', 0.7395472526550293), ('general', 0.6883535981178284), ('etc', 0.6875367164611816), ('borg', 0.6808226108551025), ('numerical', 0.6750637292861938), ('selection', 0.6706938743591309), ('including', 0.6690239310264587), ('below', 0.6681692004203796), ('within', 0.6670719981193542), ('matching', 0.667030394077301)]\n",
      "\n",
      "Closest words to \"if\":\n",
      "[('elif', 0.7820208072662354), ('not', 0.7155658006668091), ('and', 0.6704928874969482), ('both', 0.6607910990715027), ('force', 0.6586593389511108), ('auto', 0.6574268341064453), ('else', 0.6554098725318909), ('otherwise', 0.6551583409309387), ('enabled', 0.6409156918525696), ('then', 0.6270421743392944)]\n",
      "\n",
      "Similarity of \"math\" and \"numpy\":\n",
      "0.5131376\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(lines_of_code, window=5, min_count=1)\n",
    "\n",
    "print(\"Closest words to \\\"for\\\":\")\n",
    "print(model.wv.most_similar(\"for\"))\n",
    "\n",
    "print(\"\\nClosest words to \\\"if\\\":\")\n",
    "print(model.wv.most_similar(\"if\"))\n",
    "\n",
    "print(\"\\nSimilarity of \\\"math\\\" and \\\"numpy\\\":\")\n",
    "print(model.wv.similarity(\"math\", \"numpy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "with codecs.open('py_files.txt', \"r\", \"utf-8\") as f:\n",
    "    code = f.read()\n",
    "\n",
    "# get a list of all identifiers in the ast tree\n",
    "identifiers = []\n",
    "for line in code.splitlines():\n",
    "    try:\n",
    "        ast_tree = ast.parse(line)\n",
    "        for node in ast.walk(ast_tree):\n",
    "            if isinstance(node, ast.Name):\n",
    "                identifiers.append(node.id)\n",
    "    except SyntaxError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the frequency\n",
    "freq = Counter(identifiers)\n",
    "\n",
    "# find the 50 most common identifiers \n",
    "with codecs.open('most_common.txt', 'w', encoding='utf-8', errors='replace') as f:\n",
    "    for name, count in freq.most_common(50):\n",
    "        f.write(\"Name: \" + str(name) + \", count: \" + str(count) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar pairs:\n",
      "ax1 - ax2: 0.9884\n",
      "axs - ax2: 0.9662\n",
      "axs - ax1: 0.9617\n",
      "fig - axs: 0.9491\n",
      "fig - ax1: 0.9359\n",
      "fig - ax2: 0.9336\n",
      "re - sys: 0.9237\n",
      "np - plt: 0.9222\n",
      "re - pytest: 0.8869\n",
      "plt - default_manager: 0.8746\n",
      "\n",
      "Least similar pairs:\n",
      "def_gen - data: -0.1764\n",
      "x - def_gen: -0.1498\n",
      "y - def_gen: -0.0928\n",
      "type - ax2: -0.0636\n",
      "ax2 - s: -0.0496\n",
      "str - clf: -0.0474\n",
      "int - clf: -0.0402\n",
      "type - clf: -0.0382\n",
      "i8 - time: -0.0371\n",
      "fig - s: -0.0370\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "common_ids = [name for name, count in freq.most_common(50)]\n",
    "\n",
    "similarities = {}\n",
    "for id1, id2 in itertools.combinations(common_ids, 2):\n",
    "    try:\n",
    "        similarity = model.wv.similarity(id1, id2)\n",
    "        similarities[(id1, id2)] = similarity\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "print('Most similar pairs:')\n",
    "sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "for pair, similarity in sorted_similarities[:10]:\n",
    "    print(f'{pair[0]} - {pair[1]}: {similarity:.4f}')\n",
    "\n",
    "print('\\nLeast similar pairs:')\n",
    "sorted_similarities = sorted(similarities.items(), key=lambda x: x[1])\n",
    "for pair, similarity in sorted_similarities[:10]:\n",
    "    print(f'{pair[0]} - {pair[1]}: {similarity:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "739570d89e7439ec0d18c2bbd9de2de4f36d34edd9045fc0063c6b5497fdd408"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
